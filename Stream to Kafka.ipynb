{"cells":[{"cell_type":"markdown","source":["# Stream Data to Event Hubs Using Kafka\n\nThis notebook illustrates how Spark Streaming can be used to send data to a Kafka sink, specifically an Azure Event Hub using the Kafka API.\n\nFirst, we will create a dataframe with some made-up data.  Then we will run a loop that repeatedly, randomly samples from that dataframe and sends the sample to the Event Hub.  The process will continue to run until you turn it off;\ndon't forget to shut it down when you're done to save costs.\n\n**NOTE:** Your event hub must be on the \"Standard\" pricing tier or higher.  The \"Basic\" tier does not support Kafka protocol."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"75e652bf-3c3d-401a-8200-b5287c3a664c"}}},{"cell_type":"code","source":["# SETTINGS\n# You need to customize the values below to match your environment\n\n\n# This is the connection string for your event hub instance or your event hub namespace.  You can retrieve this from the Azure portal.  Note that it must include a Shared Access Key with \"Send\" permission.\nevent_hub_connection_string = \"<YOUR_EVENT_HUB_CONNECTION_STRING>\"\n\n\n# This is the name of the event hub instance within your event hub namespace\ntopic_name = \"<YOUR_EVENT_HUB_NAME>\"\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"350ce85f-037f-4f43-8ed2-050ac8a57708"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["import pandas as pd\n\ndfs = [\n    pd.DataFrame(['red', 'blue', 'green', 'yellow', 'orange'], columns = ['color']),\n    pd.DataFrame(['north', 'south', 'east', 'west'], columns = ['region']),\n    pd.DataFrame(['basic', 'premium'], columns = ['tier']),\n    pd.DataFrame(['glossy', 'matte'], columns = ['finish']),\n    pd.DataFrame(['small', 'medium', 'large'], columns = ['size']),\n    pd.DataFrame(['delivery', 'pickup'], columns = ['transit']),\n    pd.DataFrame(['cardboard', 'plastic'], columns = ['packaging']),\n    pd.DataFrame(['none', 'chrome', 'gold'], columns = ['accent']),\n    pd.DataFrame([1, 1.25, 1.5, 1.75, 2, 2.5, 3, 4], columns = ['factor'])\n]\n\nfor d in dfs:\n    d['key'] = 0\n\nexpected_row_count = 1\nfor d in dfs:\n    expected_row_count = expected_row_count * d.shape[0]\n\nprint(f'Expected Rows: {expected_row_count:,}')    \n\ndf = dfs[0]\n\nfor i in range(1, len(dfs)):\n    df = df.merge(dfs[i], on='key', how='outer')\n\nprint(f'  Actual Rows: {df.shape[0]:,}')    \n    \ndf = spark.createDataFrame(df).cache()\ndisplay(df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"da9c10f8-0ce0-4cf1-9445-db7359f84df6"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["import time\nimport pyspark.sql.functions as pyf\nfrom datetime import datetime, timedelta\nfrom random import gauss\n\ndef send_new_rows_to_kafka(now):\n  sasl_config = f'kafkashaded.org.apache.kafka.common.security.plain.PlainLoginModule required username=\"$ConnectionString\" password=\"{event_hub_connection_string}\";'\n\n  rows_in_df = df.count()\n    \n  while (True):\n    new_now = datetime.now()\n\n    row_count = abs(int(gauss(75, 25)))\n    fraction = row_count / rows_in_df\n    price_fuzz = abs(int(gauss(50, 30)))\n    \n    slice_df = (df\n                    .sample(True, fraction)\n                    .withColumn('sales', pyf.rand() * pyf.lit(price_fuzz) * pyf.col('factor'))\n                    .withColumn('sales', pyf.round(pyf.col('sales'), 2))\n                    .drop(*['factor', 'key'])\n                    .withColumn('transaction_id', pyf.monotonically_increasing_id())\n                    .selectExpr(\"CAST(transaction_id AS string) AS key\", \"to_json(struct(*)) AS value\")                    \n                    .cache()\n                )\n\n    row_count = slice_df.count()\n    \n    slice_df \\\n      .write \\\n      .format(\"kafka\") \\\n      .option(\"kafka.bootstrap.servers\", \"ehns-stream-dataflow.servicebus.windows.net:9093\") \\\n      .option(\"kafka.security.protocol\", \"SASL_SSL\") \\\n      .option(\"kafka.sasl.mechanism\", \"PLAIN\") \\\n      .option(\"kafka.sasl.jaas.config\", sasl_config) \\\n      .option(\"topic\", topic_name) \\\n      .save()\n    \n    print(f\"Processed {row_count:,} rows between   {now}   and   {new_now}\")\n\n    now = new_now + timedelta(seconds = 0.000001)\n    time.sleep( abs(gauss(10, 4)) )\n  \nsend_new_rows_to_kafka(datetime.now() - timedelta(seconds = 5))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4da15a23-da47-4e8f-87ff-04d2d44e2e06"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Stream to Kafka","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":456416151995312}},"nbformat":4,"nbformat_minor":0}
